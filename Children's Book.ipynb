{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer, hashing_trick, one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = '/Users/Benjy/Documents/Modelling/CBTest/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readWords(filename):\n",
    "    #split file into separate words\n",
    "    with tf.gfile.GFile(filename,\"rb\") as f:\n",
    "        return f.read().decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildVocab(filename):\n",
    "    #identify each unique word and give it a unique integer\n",
    "    data = readWords(filename)\n",
    "    \n",
    "    counter = collections.Counter(data) #create counter object around words in file\n",
    "    \n",
    "    countPairs = sorted(counter.items(),key=lambda x: (-x[1], x[0])) #creates tuples of each word and its frequency\n",
    "                                                                       #and then sorts it in descending order of\n",
    "                                                                            #frequency\n",
    "    \n",
    "    words, _ = list(zip(*countPairs)) #create two lists: first list is list of words, second list is their frequencies\n",
    "    wordToId = dict(zip(words,range(len(words)))) # create dictionary that has words as keys and individual values\n",
    "                                                    #for each word as its value\n",
    "                                                        #most frequent words have lowest ID numbers\n",
    "    return wordToId\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileToWordIds(filename, wordToId):\n",
    "    #Given a word, return the value of that word\n",
    "    data = readWords(filename)\n",
    "    return [wordToId[word] for word in data if word in wordToId]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    #get the data paths\n",
    "    trainPath = os.path.join(dataPath, 'cbt_train.txt')\n",
    "    valPath = os.path.join(dataPath, 'cbt_valid.txt')\n",
    "    testPath = os.path.join(dataPath, 'cbt_test.txt')\n",
    "    \n",
    "    wordToId = buildVocab(trainPath)#get dictionary of each word and unique integer\n",
    "    trainData = fileToWordIds(trainPath,wordToId) #get unique integer of each word in training data\n",
    "    valData = fileToWordIds(valPath,wordToId) #get unique integer of each word in training data\n",
    "    testData = fileToWordIds(testPath,wordToId) #get unique integer of each word in training data\n",
    "    vocabSize = len(wordToId) # get length of vocab\n",
    "    reverseDict = dict(zip(wordToId.values(),wordToId.keys()))#get reverse dictionary so can translate integer to word                     \n",
    "    \n",
    "    print(trainData[:5])\n",
    "    #print(wordToId,'wordToId')\n",
    "    print(vocabSize,'vocabSize')\n",
    "    print(\" \".join([reverseDict[x] for x in trainData[:10]]))\n",
    "    \n",
    "    return trainData, valData, testData, vocabSize, reverseDict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52167, 119, 45977, 2089, 14287]\n",
      "68032 vocabSize\n",
      "_BOOK_TITLE_ : Andrew_Lang___Prince_Prigio.txt.out<eos>CHAPTER I. -LCB- Chapter heading picture : p1.jpg\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data, vocabulary, reversed_dictionary = loadData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glad enough to be the mother of a little prince\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join([reversed_dictionary[x] for x in train_data[100:110]])) #test out reverse dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Generator for Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasBatchGenerator():\n",
    "    \n",
    "    def __init__(self,data,numSteps,batchSize,vocabulary,skipSteps):\n",
    "        self.data = data #input data\n",
    "        self.numSteps = numSteps #number of words inputting to model\n",
    "        self.batchSize = batchSize #size of batch putting into model\n",
    "        self.vocabulary = vocabulary #size of vocab\n",
    "        self.skipSteps = skipSteps #number of words to skip between each batch\n",
    "        self.current_idx = 0 #index to keep track of location in input data\n",
    "        \n",
    "    def generate(self):\n",
    "        #create a generator function to generate the batches of input data\n",
    "        x = np.zeros((self.batchSize,self.numSteps)) #input data will have dimensions [batch size, number of words]\n",
    "        y = np.zeros((self.batchSize,self.numSteps,self.vocabulary))#training labels will have \n",
    "                                                                    #[batch Size, number of words, vocab size]\n",
    "                                                                        #as the labels will have to be one-hot encoded\n",
    "        while True:\n",
    "            for i in range(self.batchSize): #loop through number of samples we want in the batch\n",
    "                if self.current_idx + self.numSteps >= len(self.data): #if the current index is larger than length\n",
    "                                                                            #of data we need to reset it to 0\n",
    "                    self.current_idx = 0\n",
    "                x[i,:] = self.data[self.current_idx:self.current_idx+self.numSteps]\n",
    "                #x array will be filled from input data from the current index to current index + number of steps\n",
    "                tempY = self.data[self.current_idx+1:self.current_idx+self.numSteps+1]\n",
    "                #a temporary y array will just be the x array but one word ahead, as we are predicting next\n",
    "                #words in the sequence\n",
    "                y[i,:,:] = keras.utils.to_categorical(tempY,num_classes=self.vocabulary)\n",
    "                #turn the tempY array into one-hot encoding \n",
    "                self.current_idx += self.skipSteps\n",
    "                #update the current index according to how large our skip step parameter is\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSteps = 50\n",
    "batchSize = 20\n",
    "skipSteps = numSteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataGen = KerasBatchGenerator(train_data,numSteps,batchSize,vocabulary,skipSteps)\n",
    "valDataGen =KerasBatchGenerator(valid_data,numSteps,batchSize,vocabulary,skipSteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 250)           17008000  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 250)           501000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50, 250)           501000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 250)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 68032)         17076032  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50, 68032)         0         \n",
      "=================================================================\n",
      "Total params: 35,086,032\n",
      "Trainable params: 35,086,032\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hiddenSize = 250\n",
    "useDropout = True\n",
    "model = keras.Sequential() #create model using sequential constructor\n",
    "model.add(keras.layers.Embedding(vocabulary,hiddenSize,input_length =numSteps)) #add embedding layer, as its first layer\n",
    "                                                                    #we have to describe the size of input \n",
    "                                                                        #i.e. number of words\n",
    "model.add(keras.layers.LSTM(hiddenSize,return_sequences=True))\n",
    "model.add(keras.layers.LSTM(hiddenSize,return_sequences=True))\n",
    "if useDropout: #drop out is a tool to prevent overfitting, randomly turns off nodes during training\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Dense(vocabulary))) #add dense layer for each of the time steps in numSteps\n",
    "model.add(keras.layers.Activation('softmax'))#use softmax activation\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "#checkpoint saves the model after each epoch\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(filepath=dataPath + '/model-{epoch:02d}.hdf5', verbose=1)\n",
    "numEpoch = 40\n",
    "model.fit_generator(trainDataGen.generate(), len(train_data)//(batchSize*numSteps), numEpoch,\n",
    "                        validation_data=valDataGen.generate(),\n",
    "                        validation_steps=len(valid_data)//(batchSize*numSteps), callbacks=[checkpointer])\n",
    "\n",
    "#this final line runs the model on the training data for a certain number of epochs\n",
    "#the formula len(train_data//batch_size*num_steps) ensures that the entirety of the data is passed over\n",
    "#during each epoch. This formula is for the number of iterations to run for each epoch.\n",
    "#It also runs the validation data on the model after each epoch, and accuracy from this data will be returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('CBModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
